# Notes on Attention Mechanisms

- Scaled dot-product attention improves gradient stability.
- Multi-head structures capture complementary relationships.
- Key challenge: quadratic complexity with sequence length.
